{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d5750b-6f06-4355-ae42-3b4a766f6f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d960c33e-623b-4490-b509-0e894684f0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n",
    "df_results_pd = df_results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b0bed3-201f-42d6-b1ca-e84f1be22427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2cbab92-3b5b-4b89-8c16-02e7ba030b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Build any model of your choice with tunable hyperparameters\n",
    "\n",
    "2. Create an experiment setup where - for each run - you log:\n",
    "\n",
    "- the hyperparameters used in the model\n",
    "- the model itself\n",
    "- every possible metric from the model you chose\n",
    "- at least two artifacts (plots, or csv files)\n",
    "\n",
    "3. Track your MLFlow experiment and run at least 10 experiments with different parameters each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd842ba-7978-4faf-acae-8f402f1570de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  scored = whether the driver earned points\n",
    "\n",
    "df_results_pd[\"scored\"] = (df_results_pd[\"points\"].astype(float) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53040baa-10c7-473a-ad1c-2cb44a3fe9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = [\"grid\", \"laps\", \"milliseconds\", \"constructorId\", \"rank\", \"positionOrder\"]\n",
    "\n",
    "# Replace '\\N' with NaN across the whole dataframe\n",
    "df_results_pd.replace('\\\\N', np.nan, inplace=True)\n",
    "\n",
    "# Force all features to be numeric\n",
    "for col in features:\n",
    "    df_results_pd[col] = pd.to_numeric(df_results_pd[col], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in features or target\n",
    "df_model = df_results_pd.dropna(subset=features + [\"scored\"])\n",
    "\n",
    "# Split into X and y\n",
    "X = df_model[features]\n",
    "y = df_model[\"scored\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3d1a4b-6822-42df-92b0-4efc4d8a9b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize feature values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "n_estimators_list = [50, 100, 150]\n",
    "max_depth_list = [3, 5, 10, None]\n",
    "\n",
    "# Run experiments with different hyperparameter combinations\n",
    "for n in n_estimators_list:\n",
    "    for depth in max_depth_list:\n",
    "        with mlflow.start_run():\n",
    "            clf = RandomForestClassifier(n_estimators=n, max_depth=depth, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Log hyperparameters and metric\n",
    "            mlflow.log_param(\"n_estimators\", n)\n",
    "            mlflow.log_param(\"max_depth\", depth)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "            # ✅ Log confusion matrix as an artifact\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(cm)\n",
    "            disp.plot()\n",
    "            artifact_path = f\"/tmp/cm_{n}_{depth}.png\"\n",
    "            plt.savefig(artifact_path)\n",
    "            mlflow.log_artifact(artifact_path)\n",
    "            plt.close()  # prevent memory buildup\n",
    "\n",
    "            # Log the model\n",
    "            mlflow.sklearn.log_model(clf, \"model\")\n",
    "\n",
    "            print(f\"Run: n_estimators={n}, max_depth={depth}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d8d3b5-f21b-4f62-ac6f-70f090c77f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Select your best model run and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a64843-e2a1-4129-8e6f-3ffab04e7219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "experiment_id = \"2163228893731700\"\n",
    "\n",
    "# Search for best F1\n",
    "runs = client.search_runs(experiment_ids=experiment_id, order_by=[\"metrics.f1_score DESC\"])\n",
    "\n",
    "best_run = runs[0]\n",
    "print(\"Best run ID:\", best_run.info.run_id)\n",
    "print(\"F1 score:\", best_run.data.metrics[\"f1_score\"])\n",
    "print(\"Params:\", best_run.data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d562b9-8b2a-4474-ab80-0a7e5d8e6ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**WHY BEST:**\n",
    "This model had the highest F1 score of 0.9801 using 150 trees and no max depth. It performed better than others because it had more trees to capture patterns and wasn’t limited in how deep each tree could grow. This helped it make more accurate predictions compared to models with fewer trees or limited depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7885d8a4-2664-4eba-814b-53e63e1d5d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HW4 training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
